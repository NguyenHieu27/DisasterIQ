{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e30131-c2bd-4ed8-ad91-aa11329c85db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating insight: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Root=1-67ba953b-2f5d8d1e5005df4d3cf9d213;ab5f08ce-44f4-4d9d-8187-d67075f1c129)\n",
      "\n",
      "Invalid username or password.\n",
      "Generated Insight:\n",
      "Unable to generate insight at this time\n"
     ]
    }
   ],
   "source": [
    "# agents/ai_explanatory_agent.py\n",
    "from huggingface_hub import InferenceClient\n",
    "from utils.config import HUGGINGFACE_API_KEY \n",
    "import time\n",
    "\n",
    "class AIExplanatoryAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize with free Hugging Face model (using random available model)\n",
    "        self.client = InferenceClient()\n",
    "        self.template = \"\"\"Turn these data points into insurance insights:\n",
    "        \n",
    "        Disaster Event: {disaster_event}\n",
    "        Claim Surge: {claim_surge}\n",
    "        Economic Factors: {economic_factors}\n",
    "        \n",
    "        Insight:\"\"\"\n",
    "    \n",
    "    def generate_insight(self, disaster_event, claim_surge, economic_factors):\n",
    "        \"\"\"\n",
    "        Generates natural language explanations from structured data\n",
    "        \"\"\"\n",
    "        prompt = self.template.format(\n",
    "            disaster_event=disaster_event,\n",
    "            claim_surge=claim_surge,\n",
    "            economic_factors=economic_factors\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Get response from Hugging Face's free inference API\n",
    "            response = self.client.text_generation(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                model=\"mistralai/Mistral-7B-Instruct-v0.2\"  # Free model good for text generation\n",
    "            )\n",
    "            return response.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating insight: {e}\")\n",
    "            return \"Unable to generate insight at this time\"\n",
    "\n",
    "# Example usage (you can test this)\n",
    "if __name__ == \"__main__\":\n",
    "    agent = AIExplanatoryAgent()\n",
    "    \n",
    "    sample_data = {\n",
    "        \"disaster_event\": \"Category 4 Hurricane in Florida\",\n",
    "        \"claim_surge\": \"35% increase in auto claims\",\n",
    "        \"economic_factors\": \"Unemployment rate up 2%\"\n",
    "    }\n",
    "    \n",
    "    insight = agent.generate_insight(**sample_data)\n",
    "    print(\"Generated Insight:\")\n",
    "    print(insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b34ed1-d61a-4555-9329-e6eb071062ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Insight:\n",
      "Category \n",
      "        A Category 4 Hurricane in Florida (last 72 hours) triggered a 35% increase in auto claims, further strained by a 2% rise in unemployment which has reduced financial resilience in the region.\n"
     ]
    }
   ],
   "source": [
    "# agents/ai_explanatory_agent.py\n",
    "import time\n",
    "from typing import Optional\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "#from utils.config import HUGGINGFACE_API_KEY\n",
    "\n",
    "class InsightRequest(BaseModel):\n",
    "    disaster_event: str\n",
    "    claim_surge: str\n",
    "    economic_factors: str\n",
    "    region: Optional[str] = None\n",
    "    time_window: Optional[str] = None\n",
    "\n",
    "class AIExplanatoryAgent:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                 use_cache: bool = True,\n",
    "                 max_retries: int = 3):\n",
    "        \n",
    "        self.client = InferenceClient(token=HUGGINGFACE_API_KEY)\n",
    "        self.model_name = model_name\n",
    "        self.cache = {} if use_cache else None\n",
    "        self.max_retries = max_retries\n",
    "        self.rate_limit_delay = 5  # seconds\n",
    "\n",
    "        # Enhanced system prompt with formatting guidance\n",
    "        self.system_prompt = \"\"\"You are an insurance risk analyst. Convert these data points into professional insights following these rules:\n",
    "        \n",
    "        1. Start with the primary impact\n",
    "        2. Mention secondary economic factors\n",
    "        3. Keep insights under 2 sentences\n",
    "        4. Use professional tone but avoid jargon\n",
    "        \n",
    "        Examples:\n",
    "        Good: \"Category 4 Hurricane in Florida (past 72 hours) caused 35% auto claim surge, exacerbated by 2% unemployment rise reducing financial resilience.\"\n",
    "        Bad: \"There was a hurricane and some claims.\"\n",
    "        \n",
    "        Analysis Data:\n",
    "        \"\"\"\n",
    "\n",
    "    def generate_insight(self, request: InsightRequest) -> str:\n",
    "        \"\"\"Generate insight with retries and caching\"\"\"\n",
    "        cache_key = f\"{request.disaster_event}-{request.claim_surge}\"\n",
    "        \n",
    "        if self.cache and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        prompt = self._build_prompt(request)\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.text_generation(\n",
    "                    prompt,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    model=self.model_name\n",
    "                )\n",
    "                \n",
    "                insight = self._postprocess_response(response)\n",
    "                \n",
    "                if self.cache is not None:\n",
    "                    self.cache[cache_key] = insight\n",
    "                \n",
    "                return insight\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.rate_limit_delay ** (attempt + 1))\n",
    "                else:\n",
    "                    raise RuntimeError(\"Failed to generate insight after retries\")\n",
    "\n",
    "    def _build_prompt(self, request: InsightRequest) -> str:\n",
    "        \"\"\"Construct structured prompt\"\"\"\n",
    "        return f\"\"\"{self.system_prompt}\n",
    "        \n",
    "        [Disaster Event]\n",
    "        {request.disaster_event}\n",
    "        \n",
    "        [Claim Impact]\n",
    "        {request.claim_surge}\n",
    "        \n",
    "        [Economic Context]\n",
    "        {request.economic_factors}\n",
    "        \n",
    "        [Region]\n",
    "        {request.region or 'N/A'}\n",
    "        \n",
    "        [Time Window]\n",
    "        {request.time_window or 'Recent'}\n",
    "        \n",
    "        Professional Insight:\"\"\"\n",
    "\n",
    "    def _postprocess_response(self, response: str) -> str:\n",
    "        \"\"\"Clean and validate responses\"\"\"\n",
    "        # Remove special tokens and empty lines\n",
    "        clean_response = response.replace(\"<pad>\", \"\")\\\n",
    "                                 .replace(\"</s>\", \"\")\\\n",
    "                                 .strip()\n",
    "        \n",
    "        # Truncate to complete sentence\n",
    "        last_period = clean_response.rfind('.')\n",
    "        if last_period != -1:\n",
    "            return clean_response[:last_period+1]\n",
    "            \n",
    "        return clean_response[:200]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = AIExplanatoryAgent(\n",
    "        model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    request = InsightRequest(\n",
    "        disaster_event=\"Category 4 Hurricane in Florida\",\n",
    "        claim_surge=\"35% increase in auto claims\",\n",
    "        economic_factors=\"Unemployment rate up 2%\",\n",
    "        region=\"Florida\",\n",
    "        time_window=\"Last 72 hours\"\n",
    "    )\n",
    "\n",
    "    print(\"Generated Insight:\")\n",
    "    print(agent.generate_insight(request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd5879-9a9c-4e7f-bd5e-38c0240b3522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
